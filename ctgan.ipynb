{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.nn import BatchNorm1d, Dropout, LeakyReLU, Linear, Module, ReLU, Sequential\n",
    "from torch.nn import functional as F\n",
    "# import octgan.synthesizers as synthesizers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Discriminator(Module):\n",
    "    def __init__(self, input_dim, dis_dims, pack=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        dim = input_dim * pack\n",
    "        self.pack = pack\n",
    "        self.packdim = dim\n",
    "        seq = []\n",
    "        for item in list(dis_dims):\n",
    "            seq += [\n",
    "                Linear(dim, item),\n",
    "                LeakyReLU(0.2),\n",
    "                Dropout(0.5)\n",
    "            ]\n",
    "            dim = item\n",
    "        seq += [Linear(dim, 1)]\n",
    "        self.seq = Sequential(*seq)\n",
    "\n",
    "    def forward(self, input):\n",
    "        assert input.size()[0] % self.pack == 0\n",
    "        return self.seq(input.view(-1, self.packdim))\n",
    "\n",
    "\n",
    "class Residual(Module):\n",
    "    def __init__(self, i, o):\n",
    "        super(Residual, self).__init__()\n",
    "        self.fc = Linear(i, o)\n",
    "        self.bn = BatchNorm1d(o)\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        out = self.fc(input)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        return torch.cat([out, input], dim=1)\n",
    "\n",
    "\n",
    "class Generator(Module):\n",
    "    def __init__(self, embedding_dim, gen_dims, data_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        dim = embedding_dim\n",
    "        seq = []\n",
    "        for item in list(gen_dims):\n",
    "            seq += [\n",
    "                Residual(dim, item)\n",
    "            ]\n",
    "            dim += item\n",
    "        seq.append(Linear(dim, data_dim))\n",
    "        self.seq = Sequential(*seq)\n",
    "\n",
    "    def forward(self, input):\n",
    "        data = self.seq(input)\n",
    "        return data\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_activate(data, output_info):\n",
    "    data_t = []\n",
    "    st = 0\n",
    "    for item in output_info:\n",
    "        if item[1] == 'tanh':\n",
    "            ed = st + item[0]\n",
    "            data_t.append(torch.tanh(data[:, st:ed]))\n",
    "            st = ed\n",
    "        elif item[1] == 'softmax':\n",
    "            ed = st + item[0]\n",
    "            data_t.append(F.gumbel_softmax(data[:, st:ed], tau=0.2))\n",
    "            st = ed\n",
    "        else:\n",
    "            assert 0\n",
    "    return torch.cat(data_t, dim=1)\n",
    "\n",
    "\n",
    "def random_choice_prob_index(a, axis=1):\n",
    "    r = np.expand_dims(np.random.rand(a.shape[1 - axis]), axis=axis)\n",
    "    return (a.cumsum(axis=axis) > r).argmax(axis=axis)\n",
    "\n",
    "\n",
    "class Cond(object):\n",
    "    def __init__(self, data, output_info):\n",
    "        self.model = []\n",
    "\n",
    "        st = 0\n",
    "        skip = False\n",
    "        max_interval = 0\n",
    "        counter = 0\n",
    "        for item in output_info:\n",
    "            if item[1] == 'tanh':\n",
    "                st += item[0]\n",
    "                skip = True\n",
    "                continue\n",
    "            elif item[1] == 'softmax':\n",
    "                if skip:\n",
    "                    skip = False\n",
    "                    st += item[0]\n",
    "                    continue\n",
    "\n",
    "                ed = st + item[0]\n",
    "                max_interval = max(max_interval, ed - st)\n",
    "                counter += 1\n",
    "                self.model.append(np.argmax(data[:, st:ed], axis=-1))\n",
    "                st = ed\n",
    "            else:\n",
    "                assert 0\n",
    "        assert st == data.shape[1]\n",
    "\n",
    "        self.interval = []\n",
    "        self.n_col = 0\n",
    "        self.n_opt = 0\n",
    "        skip = False\n",
    "        st = 0\n",
    "        self.p = np.zeros((counter, max_interval))\n",
    "        for item in output_info:\n",
    "            if item[1] == 'tanh':\n",
    "                skip = True\n",
    "                st += item[0]\n",
    "                continue\n",
    "            elif item[1] == 'softmax':\n",
    "                if skip:\n",
    "                    st += item[0]\n",
    "                    skip = False\n",
    "                    continue\n",
    "                ed = st + item[0]\n",
    "                tmp = np.sum(data[:, st:ed], axis=0)\n",
    "                tmp = np.log(tmp + 1)\n",
    "                tmp = tmp / np.sum(tmp)\n",
    "                self.p[self.n_col, :item[0]] = tmp\n",
    "                self.interval.append((self.n_opt, item[0]))\n",
    "                self.n_opt += item[0]\n",
    "                self.n_col += 1\n",
    "                st = ed\n",
    "            else:\n",
    "                assert 0\n",
    "        self.interval = np.asarray(self.interval)\n",
    "\n",
    "    def sample(self, batch):\n",
    "        if self.n_col == 0:\n",
    "            return None\n",
    "        batch = batch\n",
    "        idx = np.random.choice(np.arange(self.n_col), batch)\n",
    "\n",
    "        vec1 = np.zeros((batch, self.n_opt), dtype='float32')\n",
    "        mask1 = np.zeros((batch, self.n_col), dtype='float32')\n",
    "        mask1[np.arange(batch), idx] = 1\n",
    "        opt1prime = random_choice_prob_index(self.p[idx])\n",
    "        opt1 = self.interval[idx, 0] + opt1prime\n",
    "        vec1[np.arange(batch), opt1] = 1\n",
    "\n",
    "        return vec1, mask1, idx, opt1prime\n",
    "\n",
    "    def sample_zero(self, batch):\n",
    "        if self.n_col == 0:\n",
    "            return None\n",
    "        vec = np.zeros((batch, self.n_opt), dtype='float32')\n",
    "        idx = np.random.choice(np.arange(self.n_col), batch)\n",
    "        for i in range(batch):\n",
    "            col = idx[i]\n",
    "            pick = int(np.random.choice(self.model[col]))\n",
    "            vec[i, pick + self.interval[col, 0]] = 1\n",
    "        return vec\n",
    "\n",
    "\n",
    "def cond_loss(data, output_info, c, m):\n",
    "    loss = []\n",
    "    st = 0\n",
    "    st_c = 0\n",
    "    skip = False\n",
    "    for item in output_info:\n",
    "        if item[1] == 'tanh':\n",
    "            st += item[0]\n",
    "            skip = True\n",
    "\n",
    "        elif item[1] == 'softmax':\n",
    "            if skip:\n",
    "                skip = False\n",
    "                st += item[0]\n",
    "                continue\n",
    "\n",
    "            ed = st + item[0]\n",
    "            ed_c = st_c + item[0]\n",
    "            tmp = F.cross_entropy(\n",
    "                data[:, st:ed],\n",
    "                torch.argmax(c[:, st_c:ed_c], dim=1),\n",
    "                reduction='none'\n",
    "            )\n",
    "            loss.append(tmp)\n",
    "            st = ed\n",
    "            st_c = ed_c\n",
    "\n",
    "        else:\n",
    "            assert 0\n",
    "    loss = torch.stack(loss, dim=1)\n",
    "\n",
    "    return (loss * m).sum() / data.size()[0]\n",
    "\n",
    "\n",
    "class Sampler(object):\n",
    "    \"\"\"docstring for Sampler.\"\"\"\n",
    "\n",
    "    def __init__(self, data, output_info):\n",
    "        super(Sampler, self).__init__()\n",
    "        self.data = data\n",
    "        self.model = []\n",
    "        self.n = len(data)\n",
    "\n",
    "        st = 0\n",
    "        skip = False\n",
    "        for item in output_info:\n",
    "            if item[1] == 'tanh':\n",
    "                st += item[0]\n",
    "                skip = True\n",
    "            elif item[1] == 'softmax':\n",
    "                if skip:\n",
    "                    skip = False\n",
    "                    st += item[0]\n",
    "                    continue\n",
    "                ed = st + item[0]\n",
    "                tmp = []\n",
    "                for j in range(item[0]):\n",
    "                    tmp.append(np.nonzero(data[:, st + j])[0])\n",
    "                self.model.append(tmp)\n",
    "                st = ed\n",
    "            else:\n",
    "                assert 0\n",
    "        assert st == data.shape[1]\n",
    "\n",
    "    def sample(self, n, col, opt):\n",
    "        if col is None:\n",
    "            idx = np.random.choice(np.arange(self.n), n)\n",
    "            return self.data[idx]\n",
    "        idx = []\n",
    "        for c, o in zip(col, opt):\n",
    "            idx.append(np.random.choice(self.model[c][o]))\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "def calc_gradient_penalty(netD, real_data, fake_data, device='cpu', pac=10, lambda_=10):\n",
    "    alpha = torch.rand(real_data.size(0) // pac, 1, 1, device=device)\n",
    "    alpha = alpha.repeat(1, pac, real_data.size(1))\n",
    "    alpha = alpha.view(-1, real_data.size(1))\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "\n",
    "    disc_interpolates = netD(interpolates)\n",
    "\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=disc_interpolates, inputs=interpolates,\n",
    "        grad_outputs=torch.ones(disc_interpolates.size(), device=device),\n",
    "        create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    gradient_penalty = (\n",
    "        (gradients.view(-1, pac * real_data.size(1)).norm(2, dim=1) - 1) ** 2).mean() * lambda_\n",
    "    return gradient_penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bigdyl/anaconda3/envs/diffusion/lib/python3.9/site-packages/sklearn/mixture/_base.py:274: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/bigdyl/anaconda3/envs/diffusion/lib/python3.9/site-packages/sklearn/mixture/_base.py:274: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/bigdyl/anaconda3/envs/diffusion/lib/python3.9/site-packages/sklearn/mixture/_base.py:274: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/bigdyl/anaconda3/envs/diffusion/lib/python3.9/site-packages/sklearn/mixture/_base.py:274: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/bigdyl/anaconda3/envs/diffusion/lib/python3.9/site-packages/sklearn/mixture/_base.py:274: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn(\n",
      "/home/bigdyl/anaconda3/envs/diffusion/lib/python3.9/site-packages/sklearn/mixture/_base.py:274: ConvergenceWarning: Initialization 1 did not converge. Try different init parameters, or increase max_iter, tol or check for degenerate data.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from data import load_dataset\n",
    "from transformers import BGMTransformer\n",
    "train, test, meta, categorical_columns, ordinal_columns = load_dataset(\"adult\")\n",
    "\n",
    "transformer = BGMTransformer()\n",
    "transformer.fit(train, categorical_columns, ordinal_columns)\n",
    "train_data = transformer.transform(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if meta['problem_type'] == 'binary_classification':\n",
    "    metric = 'binary_f1'\n",
    "elif meta['problem_type'] == 'multiclass_classification':\n",
    "    metric = 'macro_f1'\n",
    "else:\n",
    "    metric = 'r2'\n",
    "\n",
    "data_sampler = Sampler(train_data, transformer.output_info)\n",
    "data_dim = transformer.output_dim\n",
    "cond_generator = Cond(train_data, transformer.output_info)\n",
    "\n",
    "embedding_dim = 64\n",
    "gen_dim = [256, 256]\n",
    "dis_dim = [256, 256]\n",
    "lr = 2e-03\n",
    "batch_size = 1024\n",
    "\n",
    "generator = Generator(\n",
    "    embedding_dim + cond_generator.n_opt,\n",
    "    gen_dim,\n",
    "    data_dim).to('cuda')\n",
    "\n",
    "discriminator = Discriminator(\n",
    "    data_dim + cond_generator.n_opt,\n",
    "    dis_dim).to('cuda')\n",
    "\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "\n",
    "if len(train_data) <= batch_size:\n",
    "    batch_size = (len(train_data) // 10)*10\n",
    "\n",
    "assert batch_size % 2 == 0\n",
    "mean = torch.zeros(batch_size, embedding_dim, device='cuda')\n",
    "std = mean + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36178\n",
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1020) must match the size of tensor b (1024) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m y_real \u001b[39m=\u001b[39m discriminator(real_cat)\n\u001b[1;32m     41\u001b[0m loss_d \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mmean(y_real) \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mmean(y_fake)\n\u001b[0;32m---> 42\u001b[0m pen \u001b[39m=\u001b[39m calc_gradient_penalty(discriminator, real_cat, fake_cat, device)\n\u001b[1;32m     44\u001b[0m optimizerD\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     45\u001b[0m pen\u001b[39m.\u001b[39mbackward(retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[19], line 186\u001b[0m, in \u001b[0;36mcalc_gradient_penalty\u001b[0;34m(netD, real_data, fake_data, device, pac, lambda_)\u001b[0m\n\u001b[1;32m    183\u001b[0m alpha \u001b[39m=\u001b[39m alpha\u001b[39m.\u001b[39mrepeat(\u001b[39m1\u001b[39m, pac, real_data\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m))\n\u001b[1;32m    184\u001b[0m alpha \u001b[39m=\u001b[39m alpha\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, real_data\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m))\n\u001b[0;32m--> 186\u001b[0m interpolates \u001b[39m=\u001b[39m alpha \u001b[39m*\u001b[39;49m real_data \u001b[39m+\u001b[39m ((\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m alpha) \u001b[39m*\u001b[39m fake_data)\n\u001b[1;32m    188\u001b[0m disc_interpolates \u001b[39m=\u001b[39m netD(interpolates)\n\u001b[1;32m    190\u001b[0m gradients \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgrad(\n\u001b[1;32m    191\u001b[0m     outputs\u001b[39m=\u001b[39mdisc_interpolates, inputs\u001b[39m=\u001b[39minterpolates,\n\u001b[1;32m    192\u001b[0m     grad_outputs\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mones(disc_interpolates\u001b[39m.\u001b[39msize(), device\u001b[39m=\u001b[39mdevice),\n\u001b[1;32m    193\u001b[0m     create_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, only_inputs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1020) must match the size of tensor b (1024) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "device = 'cuda'\n",
    "\n",
    "steps_per_epoch = len(train_data) // batch_size\n",
    "print(len(train_data))\n",
    "for i in range(epochs):\n",
    "    print(i)\n",
    "    for id_ in range(steps_per_epoch):\n",
    "        fakez = torch.normal(mean=mean, std=std)\n",
    "\n",
    "        condvec = cond_generator.sample(batch_size)\n",
    "        if condvec is None:\n",
    "            c1, m1, col, opt = None, None, None, None\n",
    "            real = data_sampler.sample(batch_size, col, opt)\n",
    "        else:\n",
    "            c1, m1, col, opt = condvec\n",
    "            c1 = torch.from_numpy(c1).to(device)\n",
    "            m1 = torch.from_numpy(m1).to(device)\n",
    "            fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "            perm = np.arange(batch_size)\n",
    "            np.random.shuffle(perm)\n",
    "            real = data_sampler.sample(batch_size, col[perm], opt[perm])\n",
    "            c2 = c1[perm]\n",
    "\n",
    "        fake = generator(fakez)\n",
    "        fakeact = apply_activate(fake, transformer.output_info)\n",
    "\n",
    "        real = torch.from_numpy(real.astype('float32')).to(device)\n",
    "\n",
    "        if c1 is not None:\n",
    "            fake_cat = torch.cat([fakeact, c1], dim=1).to(device)\n",
    "            real_cat = torch.cat([real, c2], dim=1).to(device)\n",
    "        else:\n",
    "            real_cat = real.to(device)\n",
    "            fake_cat = fake.to(device)\n",
    "\n",
    "        y_fake = discriminator(fake_cat)\n",
    "        y_real = discriminator(real_cat)\n",
    "\n",
    "        loss_d = -torch.mean(y_real) + torch.mean(y_fake)\n",
    "        pen = calc_gradient_penalty(discriminator, real_cat, fake_cat, device)\n",
    "\n",
    "        optimizerD.zero_grad()\n",
    "        pen.backward(retain_graph=True)\n",
    "        loss_d.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        fakez = torch.normal(mean=mean, std=std)\n",
    "        condvec = cond_generator.sample(batch_size)\n",
    "\n",
    "        if condvec is None:\n",
    "            c1, m1, col, opt = None, None, None, None\n",
    "        else:\n",
    "            c1, m1, col, opt = condvec\n",
    "            c1 = torch.from_numpy(c1).to(device)\n",
    "            m1 = torch.from_numpy(m1).to(device)\n",
    "            fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "        fake = generator(fakez)\n",
    "        fakeact = apply_activate(fake, transformer.output_info)\n",
    "\n",
    "        if c1 is not None:\n",
    "            y_fake = discriminator(torch.cat([fakeact, c1], dim=1))\n",
    "        else:\n",
    "            y_fake = discriminator(fakeact)\n",
    "\n",
    "        if condvec is None:\n",
    "            cross_entropy = 0\n",
    "        else:\n",
    "            cross_entropy = cond_loss(fake, transformer.output_info, c1, m1)\n",
    "\n",
    "        loss_g = -torch.mean(y_fake) + cross_entropy\n",
    "\n",
    "        optimizerG.zero_grad()\n",
    "        loss_g.backward()\n",
    "        optimizerG.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 252])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def sample(self, n):\n",
    "# n=18000\n",
    "starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "generator.eval()\n",
    "\n",
    "output_info = transformer.output_info\n",
    "steps = n // batch_size + 1\n",
    "# start = time.time()\n",
    "torch.cuda.synchronize()\n",
    "starter.record()\n",
    "\n",
    "data = []\n",
    "for i in range(steps):\n",
    "    mean = torch.zeros(batch_size, embedding_dim)\n",
    "    std = mean + 1\n",
    "    fakez = torch.normal(mean=mean, std=std).to(device)\n",
    "\n",
    "    condvec = cond_generator.sample_zero(batch_size)\n",
    "    if condvec is None:\n",
    "        pass\n",
    "    else:\n",
    "        c1 = condvec\n",
    "        c1 = torch.from_numpy(c1).to(device)\n",
    "        fakez = torch.cat([fakez, c1], dim=1)\n",
    "\n",
    "    fake = generator(fakez)\n",
    "    fakeact = apply_activate(fake, output_info)\n",
    "    data.append(fakeact.detach().cpu().numpy())\n",
    "\n",
    "# end = time.time()\n",
    "ender.record()\n",
    "torch.cuda.synchronize()\n",
    "curr_time = starter.elapsed_time(ender)\n",
    "\n",
    "print(f\"{curr_time} takes to sampling {n} records.\")\n",
    "# exit()\n",
    "data = np.concatenate(data, axis=0)\n",
    "data = data[:n]\n",
    "data = transformer.inverse_transform(data, None)\n",
    "\n",
    "\n",
    "# if not os.path.exists(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "#     np.savetxt(os.path.join(save_dir, f\"{sample}.csv\"), data, delimiter=',')\n",
    "\n",
    "return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
